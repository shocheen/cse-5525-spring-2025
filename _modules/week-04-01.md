---
title: Transformers (Jan 29)
---

### Content

* Self attention
* Transformer Encoder
* Transformer Decoder (Cross Attention, Masked Self Attention)
* Impact of transformers

### Slides
[Transformers]({{ site.url }}/{{ site.baseurl }}/assets/slides/lecture7.pdf)

### Reading Material 
[[Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)]

[[J&M Chapter 9](https://web.stanford.edu/~jurafsky/slp3/)]

[[Attention is all you need](https://arxiv.org/abs/1706.03762)]



