---
title: Tokenization Contd. / Masked LMs (February 7)
---

### Content

* Unigram tokenizer
* Pretraining / finetuning paradigm
* Masked LMs - BERT, RoBERTa, ELECTRA

### Slides
[Masked LMs]({{ site.url }}/{{ site.baseurl }}/assets/slides/lecture9.pdf)

### Reading Material 
[[Illustrated BERT)](https://jalammar.github.io/illustrated-bert/)]

[[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)]

[[RoBERTa](https://arxiv.org/pdf/1907.11692)]

[[ELECTRA](https://arxiv.org/abs/2003.10555)]



