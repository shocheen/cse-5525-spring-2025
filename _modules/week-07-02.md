---
title: Instruction Following (February 19)
---

### Content

* Reward Modeling
* Basics of RLHF
* Direct Preference Optimization

### Slides
[Learning from Preferences]({{ site.url }}/{{ site.baseurl }}/assets/slides/lecture13.pdf)

### Reading Material 

[[Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf)]

[[DPO](https://arxiv.org/abs/2305.18290)]

[[PPO vs DPO](https://arxiv.org/abs/2406.09279)]

[[Other resources](https://github.com/opendilab/awesome-RLHF)]




